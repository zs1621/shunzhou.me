<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    
    <title>
        LLM与本地知识库 |
        
        舜子</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="目录如下  一些基本概念 知识库为什么需要大模型 使用Llamaindex连接大模型和本地知识库 需要处理的问题  一些基本概念 embedding:   嵌入是一种向量表示，用于保留数据（例如一些文本）的内容和&#x2F;或意义的维度。在某种程度上相似的数据块往往具有更接近的嵌入向量，而不相关的数据则相距较远。  tokens: 大模型处理数据的最小单位，一般中文1个字代表一个token, 英文的话一般一">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM与本地知识库">
<meta property="og:url" content="https://shunzhou.me/2023/07/10/2023-07-10-LLM%E4%B8%8E%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93/">
<meta property="og:site_name" content="舜子">
<meta property="og:description" content="目录如下  一些基本概念 知识库为什么需要大模型 使用Llamaindex连接大模型和本地知识库 需要处理的问题  一些基本概念 embedding:   嵌入是一种向量表示，用于保留数据（例如一些文本）的内容和&#x2F;或意义的维度。在某种程度上相似的数据块往往具有更接近的嵌入向量，而不相关的数据则相距较远。  tokens: 大模型处理数据的最小单位，一般中文1个字代表一个token, 英文的话一般一">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://shunzhou.me/img/typro/image-20230625110946455.png">
<meta property="article:published_time" content="2023-07-09T22:01:11.000Z">
<meta property="article:modified_time" content="2024-02-20T00:49:26.689Z">
<meta property="article:author" content="zs1621">
<meta property="article:tag" content="知识">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="LangChain">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shunzhou.me/img/typro/image-20230625110946455.png">
    
    
    
        
            <link rel="stylesheet" href="https://shunzhou.me/css/markdown.css">
        
            <link rel="stylesheet" href="https://shunzhou.me/css/july.css">
        
    
<meta name="generator" content="Hexo 6.3.0"></head>
<body>

<div id="banner-outer" class="hidden">
    <div id="banner-image" style="background-image: url()"></div>
    <img src="/images/head_pic.jpg" id="avatar">
</div>

<div id="menu-outer">
    <div id="menu-inner">
        
            <a class="false" href="https://shunzhou.me/index.html">Home</a>
        
            <a class="false" href="https://shunzhou.me/archives/index.html">Archives</a>
        
            <a class="false" href="https://shunzhou.me/weekly/index.html">Weekly</a>
        
            <a class="false" href="https://shunzhou.me/about/index.html">About</a>
        
    </div>
</div>

<div id="content-outer" class="container">
    <div id="content-inner">
        <article id="post">
    <h1 id="post-title">LLM与本地知识库</h1>
    <time id="post-date" datetime="2023-07-09T22:01:11.000Z">
        七月 10, 2023
    </time>
    <div id="post-content" class="markdown-body">
        <p>目录如下</p>
<blockquote>
<p>一些基本概念</p>
<p>知识库为什么需要大模型</p>
<p>使用Llamaindex连接大模型和本地知识库</p>
<p>需要处理的问题</p>
</blockquote>
<h3 id="一些基本概念"><a href="#一些基本概念" class="headerlink" title="一些基本概念"></a>一些基本概念</h3><ul>
<li>embedding:   嵌入是一种向量表示，用于保留数据（例如一些文本）的内容和/或意义的维度。在某种程度上相似的数据块往往具有更接近的嵌入向量，而不相关的数据则相距较远。 </li>
<li>tokens: 大模型处理数据的最小单位，一般中文1个字代表一个token, 英文的话一般一个token=0.75个英文单词</li>
</ul>
<h3 id="知识库为什么需要大模型"><a href="#知识库为什么需要大模型" class="headerlink" title="知识库为什么需要大模型"></a>知识库为什么需要大模型</h3><p>我们来看一个日常工作中常见的工作流</p>
<ul>
<li>A:  xxxxx, 这个是什么原因啊</li>
<li>B:  稍等，我看下哈<ol>
<li>如果【大脑检索】xxxx问题出现过，根据记忆来回答问题</li>
<li>如果【大脑检索】xxxx问题出现过， 但具体结论之类的忘了，但是记录了文档,  那就需要根据关键词从【文档中检索】</li>
<li>如果【大脑检索】xxxx问题出现过， 但具体结论之类的忘了，也没有记录文档，那就需要重新根据问题排查问题了</li>
<li>如果【大脑检索】xxxx问题未出现过， 处理方案同上</li>
</ol>
</li>
</ul>
<p>上面这个工作流，从耗时来说 1, 2 &lt;&lt; 3,4</p>
<p>也是日常工作的非常常见的工作流,  那么这里的问题就转化两个步骤</p>
<ol>
<li>为你初次处理此问题的上下文，处理流程是否保存完备</li>
<li>以及当我们在搜索时, 能够较为方便的搜索到初次处理问题的文档</li>
</ol>
<p>步骤1 完全靠自己自觉了,  我们可以操作的部分就是步骤2了</p>
<p>步骤2的核心就是搜索了,  有了google/百度搜索工具之后， 人类利用知识的效率大大提升， 可以想象一下，没有搜索引擎之前我们能做什么？ 读书/向认识的咨询专家在哪-向专家请求/</p>
<p>有搜索引擎的利好，让整个数字化的流程大大加速。 回到我们的搜索场景， 其实是本地化的, 本地化的搜索逻辑基本上是 利用全文搜索去解决如ELK/confluence。</p>
<p>我们再看看全文搜索的问题， 通过关键词去找关联， 会存在2个问题</p>
<ul>
<li>关键词必须要匹配</li>
<li>关键词比较短的情况，搜索结果会特别大 </li>
</ul>
<p>那么怎么解决这个问题？ </p>
<blockquote>
<p>大模型，openai的大模型让我们看到了一个有理解能力和表达能力的超级知识体。 </p>
<ol>
<li>它掌握了几乎人类公开最精华的知识库了</li>
<li>它能够理解你的问题, 并基于它的知识库来组织语言回复你 </li>
</ol>
</blockquote>
<p>上面描述的2个特性，完美的解决上面全文索引的问题， 在搜索引擎之后， 人类利用知识的效率达到一个新的高度。  </p>
<p>其实如果使用大模型与大模型内部的知识库，其实很多什么只能解决通用性问题， 一旦问题涉及到自己的业务领域， 此时就需要把自己业务领域的知识喂给大模型。但是这里涉及几个问题</p>
<ul>
<li>业务领域数据属于公司自有数据，全部喂给大模型，有数据泄露风险</li>
<li>大模型有token限制 ， 目前的GPT-3.5-tubo-16K的限制是16k。 (业务领域的知识远远大于这个数字)</li>
</ul>
<p>那怎么解决上面的问题呢？ </p>
<blockquote>
<p>llamada-index </p>
</blockquote>
<h3 id="使用Llamaindex连接大模型和本地知识库"><a href="#使用Llamaindex连接大模型和本地知识库" class="headerlink" title="使用Llamaindex连接大模型和本地知识库"></a>使用Llamaindex连接大模型和本地知识库</h3><p><img src="https://shunzhou.me/img/typro/image-20230625110946455.png" alt="image-20230625110946455"></p>
<blockquote>
<p>llamaIndex 处理什么问题？</p>
<p>提供一系列工具去方便用户去使用llm去调用本地知识库</p>
<p>工具包含:  对于各种大模型的封装 / 对于各种本地知识库的文件文件解析 / embedding模型的支持 / 向量数据库连接的封装</p>
</blockquote>
<h4 id="典型流程"><a href="#典型流程" class="headerlink" title="典型流程"></a>典型流程</h4><ul>
<li>用户问了一个问题 </li>
<li>llamda-index将用户问题(转化为向量)与本地知识库(向量数据库存储) 匹配， 搜索出Top-k 的相关文件片段 </li>
<li>llamad-index 将搜索用户问题 + 搜索出的相关文件片段  发送给大模型 </li>
<li>大模型给出答复</li>
</ul>
<h4 id="showcode"><a href="#showcode" class="headerlink" title="showcode"></a>showcode</h4><ul>
<li><p>本地文档如下-markdown文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">### 运维常用联系人</span><br><span class="line">svn负责人:   xxx1</span><br><span class="line">gitlab负责人:  xxx2</span><br><span class="line">sonarqube负责人: xxx3</span><br></pre></td></tr></table></figure>
</li>
<li><p>代码如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">from langchain.llms import OpenAIChat</span><br><span class="line"></span><br><span class="line"># 这一步可以不需要，可以把OPENAI_API_KEY 放在环境变量里</span><br><span class="line"># os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;&#x27;</span><br><span class="line"></span><br><span class="line">from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, \</span><br><span class="line">    LLMPredictor, ServiceContext</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    llm_predictor = LLMPredictor(llm=OpenAIChat(model_name=&quot;gpt-3.5-turbo-16k-0613&quot;, temperature=0.6, verbose=True))</span><br><span class="line">    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)</span><br><span class="line">    if os.path.exists(&quot;./storage&quot;):</span><br><span class="line">        # rebuild storage context</span><br><span class="line">        storage_context = StorageContext.from_defaults(persist_dir=&#x27;./storage&#x27;)</span><br><span class="line">        # load index</span><br><span class="line">        index = load_index_from_storage(storage_context, service_context=service_context)</span><br><span class="line">    else:</span><br><span class="line">        # 这步会解析documents目录下的文件。 把文件放在documents目录下(支持txt,md,docx,pdf..)</span><br><span class="line">        documents = SimpleDirectoryReader(&#x27;documents&#x27;).load_data()</span><br><span class="line">        # 这里会把解析后的文本内容，到openai获取embeddings数据(需要访问openai)</span><br><span class="line">        index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)</span><br><span class="line">        # 解析文件后的数据存储到 storage目录下, 下次就不要再去解析</span><br><span class="line">        index.storage_context.persist()</span><br><span class="line"></span><br><span class="line">    # 开始查询</span><br><span class="line">    query_engine = index.as_query_engine()</span><br><span class="line">    # (需要访问openai)</span><br><span class="line">    res = query_engine.query(&quot;&lt;svn有问题需要找谁?&gt;&quot;)</span><br><span class="line">    print(res)</span><br><span class="line">    #回复是:  </span><br><span class="line">    # svn有问题需要找artifactory/svn运维人员xxx2</span><br></pre></td></tr></table></figure>
</li>
<li><p>整个代码最终目标是产生prompt</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Context information is below.</span><br><span class="line">---------------------</span><br><span class="line">运维过程常用联系人</span><br><span class="line"></span><br><span class="line">gitee/gitlab运维:  xxx1</span><br><span class="line"></span><br><span class="line">artifactory/svn运维:  xxx2</span><br><span class="line">---------------------</span><br><span class="line">Given the context information and not prior knowledge, answer the question: &lt;svn有问题需要找谁?&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>答复</p>
<blockquote>
<p>svn有问题需要找artifactory/svn运维人员xxx1</p>
</blockquote>
</li>
</ul>
<p> 总结来说:  通过openai的embedding将文本转化成向量 ，然后存入向量数据库， 然后通过向量相关性来找出相关的高文档片段作为上下文， 加上问题一起发送给大模型，获得最终结果</p>
<p>整个流程核心点：</p>
<ul>
<li>openai的embedding接口</li>
<li>向量数据库求top-n</li>
</ul>
<h4 id="引入向量数据库的必要性"><a href="#引入向量数据库的必要性" class="headerlink" title="引入向量数据库的必要性"></a>引入向量数据库的必要性</h4><ul>
<li>再回到全文索引的关键字搜索的两个问题</li>
</ul>
<blockquote>
<p>关键词绝对匹配</p>
<p>关键词比较短的情况，搜索结果会特别大</p>
</blockquote>
<p>举个例子我们搜索elk相关问题时， 如果我们搜索的问题中包含logstash, 假设你的文档里不包含logstash这个关键词，但包含elk这个词时。  那么关键字搜索时搜不到相关的文章。 此时如果用借助向量数据库,  logstash 与 elk 这两个词在openai-embedding模型里是必然有相关性的，那么它就能把相关连的文档搜出来， 然后再借助大模型的语言组织能力给你一个跟文档相关的答案。 引入了向量数据库，关键词绝对匹配的问题就不存在了。 </p>
<p>由于大模型有理解能力， 那么你在搜索问题时，其实像和人类一样问题就变的自然了，完整表达自己的问题就好了。 </p>
<ul>
<li>当然引入向量库主要的原因是token限制。 如果未来大模型没有token限制， 向量数据库在这个流程里就没有存在必要。 </li>
</ul>
<h4 id="开源模型支持"><a href="#开源模型支持" class="headerlink" title="开源模型支持"></a>开源模型支持</h4><p>还有一个问题时数据泄密问题。 这个llamda-index其实是支持开源的模型的。 整个流程里与openai有交互的有两个</p>
<ol>
<li>embedding 求文本的向量</li>
<li>给大模型问题， 让大模型给出答复 </li>
</ol>
<p>对应的解决办法</p>
<ol>
<li>使用Langchain的的embedding模型</li>
<li>LLMPredictor 可以换成 HuggingFaceLLMPredictor</li>
</ol>
<h3 id="还需要解决的问题"><a href="#还需要解决的问题" class="headerlink" title="还需要解决的问题"></a>还需要解决的问题</h3><p>通过上面的小demo, 我们能初步实现大模型和本地知识库的连接</p>
<p>当然还有很多其他的问题</p>
<ul>
<li><p>日常文档里，其实有很多截图类的东西</p>
<blockquote>
<p>这个目前的话，llmada-index目前在解析文档时会把图片，链接去掉 ， 那么其实求相关还是问题和文档文本的相关性。 图片信息内容就丢失了</p>
</blockquote>
</li>
<li><p>当问题得到回复时,我们还是想看完整文档, 这个需要对文档做个关联索引</p>
</li>
<li><p>如果你的业务足够垂直的话，其实openai的embedding也是不够好的。那么关联性就不够好.</p>
<blockquote>
<p>这块可能需要找领域相关的embedding模型来替代</p>
</blockquote>
</li>
<li><p>怎么将业务的知识库和问答系统中间的流程自动化</p>
<blockquote>
<p>知识库收集标准</p>
<p>知识库收集后的embedding和建立文档索引</p>
</blockquote>
</li>
</ul>

    </div>
</article>

<div>
    
    ...
    <section>
      <div id="gitalk-container"></div>
    </section>

    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '01408711d79d72f03315',
        clientSecret: '0b1e26de8a8688910ce255ee16fae48fed927aa4',
        accessToken: '997eb7f03bafcfef0d6b3030543c5a6e903fb771',
        id: md5(window.location.pathname),
        repo: 'shunzhou.me',
        owner: 'zs1621',
        admin: 'zs1621',
        distractionFreeMode: 'true'
    })
    gitalk.render('gitalk-container')
</script>
</div>

<div id="paginator">
    
</div>


    </div>
</div>

<div id="bottom-outer">
    <div id="bottom-inner">
        2019-2024 zs1621 
    </div>
</div>

<div id="to-top">
    <i class="iconfont icon-up"></i>
</div>


    
        <script src="https://shunzhou.me/js/jquery-3.4.1.min.js"></script>
    
        <script src="https://shunzhou.me/js/highlight-9.13.1.min.js"></script>
    
        <script src="https://shunzhou.me/js/transition.js"></script>
    
        <script src="https://shunzhou.me/js/smooth-scroll.min.js"></script>
    



    <script>
      $(function () {
        $('#banner-outer').addClass('fade-out')
        $('#menu-outer').addClass('fade-show')

        $('pre').each(function (i, block) {
          hljs.highlightBlock(block);
        });
      })
    </script>


<script>
  $(function () {
    $(window).scroll(function () {
      if ($(window).scrollTop() > 150) {
        $("#to-top").fadeIn();
      } else {
        $("#to-top").fadeOut();
      }
    });
    $("#to-top").click(function () {
      $("body,html").animate({scrollTop: 0}, 500);
      return false;
    })
  })
</script>
</body>
</html>
